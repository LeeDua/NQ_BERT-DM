{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import enum\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import multiprocessing\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import six\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from modules.graph_encoder import NodePosition, Graph, EdgeType, get_edge_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "checkpts/a.bin.3\n"
     ]
    }
   ],
   "source": [
    "_ = 3\n",
    "print(os.path.join(\"checkpts\",\"a.bin\"+\".\"+str(_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tensorflow as tf\n",
    "import six\n",
    "import json\n",
    "from src_nq.create_examples import NqExample\n",
    "input_path = \"./natural_questions/v1.0/sample/nq-train-sample.jsonl.gz\"\n",
    "fi = gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = []\n",
    "for line in fi:\n",
    "    if not isinstance(line, six.text_type):\n",
    "        line = line.decode(\"utf-8\")\n",
    "    json_example = json.loads(line)\n",
    "    question_tokens = json_example[\"question_tokens\"]\n",
    "    if question_tokens == None:\n",
    "        continue\n",
    "    doc_tokens = json_example[\"document_tokens\"]\n",
    "    annotation = json_example[\"annotations\"]\n",
    "    example_id = json_example[\"example_id\"]\n",
    "    la_candidates = json_example[\"long_answer_candidates\"]\n",
    "\n",
    "    examples.append(\n",
    "        NqExample(example_id=example_id,\n",
    "                    question_tokens=question_tokens,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    la_candidates=la_candidates,\n",
    "                    annotation=annotation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['in',\n",
       " 'greek',\n",
       " 'mythology',\n",
       " 'who',\n",
       " 'was',\n",
       " 'the',\n",
       " 'goddess',\n",
       " 'of',\n",
       " 'spring',\n",
       " 'growth']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "examples[0].question_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'end_byte': 102,\n",
       "  'html_token': False,\n",
       "  'start_byte': 92,\n",
       "  'token': 'Persephone'},\n",
       " {'end_byte': 104, 'html_token': False, 'start_byte': 103, 'token': '-'},\n",
       " {'end_byte': 114,\n",
       "  'html_token': False,\n",
       "  'start_byte': 105,\n",
       "  'token': 'Wikipedia'},\n",
       " {'end_byte': 58058, 'html_token': True, 'start_byte': 58005, 'token': '<H1>'},\n",
       " {'end_byte': 58068,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58058,\n",
       "  'token': 'Persephone'},\n",
       " {'end_byte': 58073,\n",
       "  'html_token': True,\n",
       "  'start_byte': 58068,\n",
       "  'token': '</H1>'},\n",
       " {'end_byte': 58565,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58561,\n",
       "  'token': 'This'},\n",
       " {'end_byte': 58573,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58566,\n",
       "  'token': 'article'},\n",
       " {'end_byte': 58576, 'html_token': False, 'start_byte': 58574, 'token': 'is'},\n",
       " {'end_byte': 58582,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58577,\n",
       "  'token': 'about'},\n",
       " {'end_byte': 58586, 'html_token': False, 'start_byte': 58583, 'token': 'the'},\n",
       " {'end_byte': 58592,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58587,\n",
       "  'token': 'Greek'},\n",
       " {'end_byte': 58600,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58593,\n",
       "  'token': 'goddess'},\n",
       " {'end_byte': 58601, 'html_token': False, 'start_byte': 58600, 'token': '.'},\n",
       " {'end_byte': 58605, 'html_token': False, 'start_byte': 58602, 'token': 'For'},\n",
       " {'end_byte': 58611,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58606,\n",
       "  'token': 'other'},\n",
       " {'end_byte': 58616,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58612,\n",
       "  'token': 'uses'},\n",
       " {'end_byte': 58617, 'html_token': False, 'start_byte': 58616, 'token': ','},\n",
       " {'end_byte': 58621, 'html_token': False, 'start_byte': 58618, 'token': 'see'},\n",
       " {'end_byte': 58732,\n",
       "  'html_token': False,\n",
       "  'start_byte': 58722,\n",
       "  'token': 'Persephone'}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "examples[0].doc_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'annotation_id': 7719528322202775345,\n",
       "  'long_answer': {'candidate_index': 58,\n",
       "   'end_byte': 84070,\n",
       "   'end_token': 965,\n",
       "   'start_byte': 81195,\n",
       "   'start_token': 809},\n",
       "  'short_answers': [{'end_byte': 82556,\n",
       "    'end_token': 838,\n",
       "    'start_byte': 81281,\n",
       "    'start_token': 814}],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "examples[0].annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'end_byte': 62898,\n",
       "  'end_token': 217,\n",
       "  'start_byte': 58761,\n",
       "  'start_token': 24,\n",
       "  'top_level': True},\n",
       " {'end_byte': 58933,\n",
       "  'end_token': 30,\n",
       "  'start_byte': 58810,\n",
       "  'start_token': 25,\n",
       "  'top_level': False},\n",
       " {'end_byte': 59089,\n",
       "  'end_token': 44,\n",
       "  'start_byte': 58933,\n",
       "  'start_token': 30,\n",
       "  'top_level': False},\n",
       " {'end_byte': 59894,\n",
       "  'end_token': 60,\n",
       "  'start_byte': 59089,\n",
       "  'start_token': 44,\n",
       "  'top_level': False},\n",
       " {'end_byte': 60156,\n",
       "  'end_token': 74,\n",
       "  'start_byte': 59894,\n",
       "  'start_token': 60,\n",
       "  'top_level': False},\n",
       " {'end_byte': 60254,\n",
       "  'end_token': 92,\n",
       "  'start_byte': 60156,\n",
       "  'start_token': 74,\n",
       "  'top_level': False},\n",
       " {'end_byte': 60446,\n",
       "  'end_token': 104,\n",
       "  'start_byte': 60355,\n",
       "  'start_token': 96,\n",
       "  'top_level': False},\n",
       " {'end_byte': 60618,\n",
       "  'end_token': 114,\n",
       "  'start_byte': 60446,\n",
       "  'start_token': 104,\n",
       "  'top_level': False},\n",
       " {'end_byte': 60763,\n",
       "  'end_token': 124,\n",
       "  'start_byte': 60618,\n",
       "  'start_token': 114,\n",
       "  'top_level': False},\n",
       " {'end_byte': 62753,\n",
       "  'end_token': 204,\n",
       "  'start_byte': 60763,\n",
       "  'start_token': 124,\n",
       "  'top_level': False}]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "examples[0].la_candidates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<src_nq.create_examples.NqExample at 0x7f0f6b1a8100>]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# part = examples[:10]\n",
    "part = examples[:1]\n",
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "tokenizer = BertTokenizer(vocab_file=\"./bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
    "is_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case=True\n",
    "max_seq_length=512\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "include_unknowns=0.03\n",
    "max_position=50\n",
    "num_threads=16\n",
    "seed=42\n",
    "start_num=-1\n",
    "end_num=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_nq.create_examples import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert(args, examples, tokenizer, is_training, cached_path):\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = []\n",
    "        for token in example.question_tokens:\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                query_tokens.append(sub_token)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[-max_query_length:]\n",
    "\n",
    "        counts = {\"Table\": 0, \"Paragraph\": 0, \"List\": 0, \"Other\": 0}\n",
    "\n",
    "        sorted_la_candidates = sorted(enumerate(example.la_candidates), key=lambda x: x[1][\"start_token\"])\n",
    "\n",
    "        tok_is_sentence_end = []\n",
    "        tok_is_paragraph_end = []\n",
    "        tok_candidate_idx = []\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = [-1] * len(example.doc_tokens)\n",
    "        orig_to_tok_end_index = [-1] * len(example.doc_tokens)\n",
    "        all_doc_tokens = []\n",
    "\n",
    "        for candidate_idx, candidate in sorted_la_candidates:\n",
    "            if not candidate[\"top_level\"]:\n",
    "                continue\n",
    "            start_index = candidate[\"start_token\"]\n",
    "            end_index = candidate[\"end_token\"]\n",
    "\n",
    "            context_tokens = []\n",
    "            context_orig_idx = []\n",
    "\n",
    "            for tok_idx in range(start_index, end_index):\n",
    "                token_item = example.doc_tokens[tok_idx]\n",
    "                if not token_item[\"html_token\"]:\n",
    "                    context_tokens.append(token_item[\"token\"].replace(\" \", \"\"))\n",
    "                    if context_tokens[-1] == \"\":\n",
    "                        context_tokens = context_tokens[:-1]\n",
    "                        continue\n",
    "                    context_orig_idx.append(tok_idx)\n",
    "\n",
    "            if len(context_tokens) == 0:\n",
    "                continue\n",
    "\n",
    "            all_doc_tokens.append(\n",
    "                \"[unused{}]\".format(\n",
    "                    get_candidate_type(example.doc_tokens[start_index][\"token\"], counts, max_position)))\n",
    "            tok_to_orig_index.append(-1)\n",
    "            tok_is_sentence_end.append(False)\n",
    "            tok_is_paragraph_end.append(False)\n",
    "            tok_candidate_idx.append(candidate_idx)\n",
    "\n",
    "            paragraph_len = 0\n",
    "            context = \" \".join(context_tokens)\n",
    "            context_sentences = nlp(context).sents\n",
    "            context_idx = 0\n",
    "            orig_to_tok_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "            for sentence in context_sentences:\n",
    "                sentence_len = 0\n",
    "                for token in sentence.string.strip().split():\n",
    "                    if len(context_tokens[context_idx]) == 0:\n",
    "                        orig_to_tok_end_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "                        context_idx += 1\n",
    "                        orig_to_tok_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "                    assert context_tokens[context_idx][:len(token)] == token\n",
    "                    context_tokens[context_idx] = context_tokens[context_idx][len(token):]\n",
    "\n",
    "                    sub_tokens = tokenizer.tokenize(token)\n",
    "\n",
    "                    all_doc_tokens += sub_tokens\n",
    "                    tok_to_orig_index += [context_orig_idx[context_idx]] * len(sub_tokens)\n",
    "                    sentence_len += len(sub_tokens)\n",
    "                    paragraph_len += len(sub_tokens)\n",
    "                if sentence_len > 0:\n",
    "                    tok_is_sentence_end += [False] * (sentence_len - 1) + [True]\n",
    "\n",
    "            orig_to_tok_end_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "            assert context_idx + 1 == len(context_tokens)\n",
    "            assert context_tokens[context_idx] == \"\"\n",
    "\n",
    "            tok_is_paragraph_end += [False] * (paragraph_len - 1) + [True]\n",
    "            tok_candidate_idx += [candidate_idx] * paragraph_len\n",
    "\n",
    "        assert len(tok_is_sentence_end) == len(tok_is_paragraph_end)\n",
    "\n",
    "        la_tok_start_position = None\n",
    "        la_tok_end_position = None\n",
    "\n",
    "        sa_tok_start_position = None\n",
    "        sa_tok_end_position = None\n",
    "\n",
    "        example_answer_type = None\n",
    "\n",
    "        if is_training:\n",
    "            annotation = example.annotation[0]\n",
    "            if annotation[\"long_answer\"][\"start_token\"] != -1:\n",
    "                la_start_token = annotation[\"long_answer\"][\"start_token\"]\n",
    "                la_end_token = annotation[\"long_answer\"][\"end_token\"] - 1\n",
    "                while orig_to_tok_index[la_start_token] == -1:\n",
    "                    la_start_token += 1\n",
    "                while orig_to_tok_end_index[la_end_token] == -1:\n",
    "                    la_end_token -= 1\n",
    "\n",
    "                assert la_start_token <= la_end_token\n",
    "\n",
    "                la_tok_start_position = orig_to_tok_index[la_start_token]\n",
    "                la_tok_end_position = orig_to_tok_end_index[la_end_token] - 1\n",
    "\n",
    "                if annotation[\"yes_no_answer\"].lower() == \"none\":\n",
    "                    example_answer_type = AnswerType.LONG\n",
    "                elif annotation[\"yes_no_answer\"].lower() == \"yes\":\n",
    "                    example_answer_type = AnswerType.YES\n",
    "                elif annotation[\"yes_no_answer\"].lower() == \"no\":\n",
    "                    example_answer_type = AnswerType.NO\n",
    "                else:\n",
    "                    assert False\n",
    "\n",
    "                if \"short_answers\" in annotation and len(annotation[\"short_answers\"]) > 0:\n",
    "                    sa_tok_start_position = len(all_doc_tokens) + 1\n",
    "                    sa_tok_end_position = -1\n",
    "                    example_answer_type = AnswerType.SHORT\n",
    "                    for short_answer in annotation[\"short_answers\"]:\n",
    "                        sa_start_token = short_answer[\"start_token\"]\n",
    "                        sa_end_token = short_answer[\"end_token\"] - 1\n",
    "\n",
    "                        sa_tok_start_position = min(sa_tok_start_position, orig_to_tok_index[sa_start_token])\n",
    "                        sa_tok_end_position = max(sa_tok_end_position, orig_to_tok_end_index[sa_end_token] - 1)\n",
    "                        assert sa_tok_start_position >= 0 and sa_tok_end_position >= 0\n",
    "                    assert la_tok_start_position <= sa_tok_start_position and la_tok_end_position >= sa_tok_end_position\n",
    "                    la_tok_start_position = sa_tok_start_position\n",
    "                    la_tok_end_position = sa_tok_end_position\n",
    "                else:\n",
    "                    sa_tok_start_position = -1\n",
    "                    sa_tok_end_position = -1\n",
    "            else:\n",
    "                example_answer_type = AnswerType.UNKNOWN\n",
    "                la_tok_start_position = -1\n",
    "                la_tok_end_position = -1\n",
    "                sa_tok_start_position = -1\n",
    "                sa_tok_end_position = -1\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        logger.info(\"ALL DOC_TOKENS %s\" %\" \".join([str(x) for x in all_doc_tokens][:50]))\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            is_sentence_end = [True] + [False] * (len(tokens) - 3) + [True] + [False]\n",
    "            is_paragraph_end = [True] + [False] * (len(tokens) - 3) + [True] + [False]\n",
    "            orig_tok_idx = [-1] * len(tokens)\n",
    "            candidate_idx = [-1] * len(tokens)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "\n",
    "                is_sentence_end.append(tok_is_sentence_end[split_token_index])\n",
    "                is_paragraph_end.append(tok_is_paragraph_end[split_token_index])\n",
    "                orig_tok_idx.append(tok_to_orig_index[split_token_index])\n",
    "                candidate_idx.append(tok_candidate_idx[split_token_index])\n",
    "\n",
    "            is_sentence_end[-1] = False\n",
    "            is_paragraph_end[-1] = False\n",
    "\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            is_sentence_end.append(True)\n",
    "            is_paragraph_end.append(True)\n",
    "            orig_tok_idx.append(-1)\n",
    "            candidate_idx.append(-1)\n",
    "\n",
    "            assert len(orig_tok_idx) == len(tokens)\n",
    "            assert len(is_sentence_end) == len(tokens)\n",
    "            assert len(is_paragraph_end) == len(tokens)\n",
    "            assert len(candidate_idx) == len(tokens)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            la_start_position = None\n",
    "            la_end_position = None\n",
    "            sa_start_position = None\n",
    "            sa_end_position = None\n",
    "            answer_type = None\n",
    "            if is_training:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (la_tok_start_position >= doc_start and\n",
    "                        la_tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "\n",
    "                if out_of_span:\n",
    "                    if random.random() > include_unknowns:\n",
    "                        continue\n",
    "                    answer_type = AnswerType.UNKNOWN\n",
    "                    la_start_position = 0\n",
    "                    la_end_position = 0\n",
    "                    sa_start_position = 0\n",
    "                    sa_end_position = 0\n",
    "                else:\n",
    "                    answer_type = example_answer_type\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    la_start_position = la_tok_start_position - doc_start + doc_offset\n",
    "                    la_end_position = la_tok_end_position - doc_start + doc_offset\n",
    "\n",
    "                    if not (sa_tok_start_position >= doc_start and sa_tok_end_position <= doc_end):\n",
    "                        sa_start_position = -1\n",
    "                        sa_end_position = -1\n",
    "                    else:\n",
    "                        sa_start_position = sa_tok_start_position - doc_start + doc_offset\n",
    "                        sa_end_position = sa_tok_end_position - doc_start + doc_offset\n",
    "\n",
    "                assert la_start_position <= la_end_position\n",
    "                assert sa_start_position <= sa_end_position\n",
    "\n",
    "            if example_index >= 0:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                print(\"token list:\",tokens[:20])\n",
    "                # print(\"token list size\", len(tokens))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens)[:20])\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()][:20]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ][:20]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids][:20]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask][:20]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids][:20]))\n",
    "                if is_training and answer_type != AnswerType.UNKNOWN:\n",
    "                    answer_text = \" \".join(tokens[la_start_position:(la_end_position + 1)][:20])\n",
    "                    logger.info(\"la_start_position: %d\" % (la_start_position))\n",
    "                    logger.info(\"la_end_position: %d\" % (la_end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "            doc_tree = get_doc_tree(is_sentence_end, is_paragraph_end, orig_tok_idx, candidate_idx)\n",
    "\n",
    "            graph, start_positions, end_positions = build_graph(doc_tree, len(is_sentence_end),\n",
    "                                                                la_start_position, la_end_position,\n",
    "                                                                sa_start_position, sa_end_position)\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_id=example.example_id,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=None,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    graph=graph,\n",
    "                    answer_type=answer_type,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   ALL DOC_TOKENS [unused0] per ##se ##phone goddess of the underworld , spring ##time , flowers and vegetation statue of per ##se ##phone with a sis ##trum . her ##ak ##lion archaeological museum , crete ab ##ode the underworld , sicily , mount olympus symbol po ##me ##gra ##nate , seeds of grain\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   unique_id: 1000000000\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   doc_span_index: 2\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_to_orig_map: 12:303 13:303 14:306 15:306 16:306 17:309 18:309 19:309 20:309 21:309 22:316 23:316 24:317 25:318 26:319 27:323 28:323 29:330 30:330 31:330\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 12256 2964 6090 10760 2964 2030 2705 7361\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_start_position: 353\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_end_position: 402\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   unique_id: 1000000001\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   doc_span_index: 3\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_to_orig_map: 12:495 13:498 14:498 15:501 16:501 17:504 18:507 19:510 20:513 21:513 22:513 23:516 24:516 25:516 26:519 27:519 28:519 29:522 30:522 31:522\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 26057 3566 2271 22822 22809 21363 18368 6090\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_start_position: 225\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_end_position: 274\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   unique_id: 1000000002\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   doc_span_index: 4\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_to_orig_map: 12:686 13:687 14:688 15:689 16:692 17:692 18:692 19:695 20:695 21:698 22:698 23:701 24:701 25:704 26:705 27:708 28:715 29:716 30:717 31:718\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 3182 1006 2265 1007 3449 10600 2483 3972\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_start_position: 97\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   la_end_position: 146\n",
      "12/30/2020 15:32:20 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'mon', '##ism', 'pan', '##the', '##ism', 'or', '##th', '##op']\n",
      "token list size 512\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'metis', 'mom', '##us', 'mor', '##pheus', 'nemesis', 'nike', 'pan']\n",
      "token list size 512\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'places', '(', 'show', ')', 'el', '##eus', '##is', 'del']\n",
      "token list size 512\n"
     ]
    }
   ],
   "source": [
    "features = test_convert(None,part, tokenizer, is_training,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   ALL DOC_TOKENS [unused0] per ##se ##phone goddess of the underworld , spring ##time , flowers and vegetation statue of per ##se ##phone with a sis ##trum . her ##ak ##lion archaeological museum , crete ab ##ode the underworld , sicily , mount olympus symbol po ##me ##gra ##nate , seeds of grain\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   unique_id: 1000000000\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   doc_span_index: 2\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_to_orig_map: 12:303 13:303 14:306 15:306 16:306 17:309 18:309 19:309 20:309 21:309 22:316 23:316 24:317 25:318 26:319 27:323 28:323 29:330 30:330 31:330\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 12256 2964 6090 10760 2964 2030 2705 7361\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_start_position: 353\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_end_position: 402\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   unique_id: 1000000001\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   doc_span_index: 3\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_to_orig_map: 12:495 13:498 14:498 15:501 16:501 17:504 18:507 19:510 20:513 21:513 22:513 23:516 24:516 25:516 26:519 27:519 28:519 29:522 30:522 31:522\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 26057 3566 2271 22822 22809 21363 18368 6090\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_start_position: 225\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_end_position: 274\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   unique_id: 1000000002\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   doc_span_index: 4\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_to_orig_map: 12:686 13:687 14:688 15:689 16:692 17:692 18:692 19:695 20:695 21:698 22:698 23:701 24:701 25:704 26:705 27:708 28:715 29:716 30:717 31:718\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 3182 1006 2265 1007 3449 10600 2483 3972\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_start_position: 97\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   la_end_position: 146\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   answer: per ##se ##phone ( / p ##ə ##r ##ˈ ##s ##ɛ ##f ##ə ##ni / ; greek : π ##ε\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   *** Example ***\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   unique_id: 1000000003\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   example_index: 0\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   doc_span_index: 10\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   tokens: [CLS] in greek mytho\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_to_orig_map: 12:1502 13:1503 14:1503 15:1503 16:1504 17:1505 18:-1 19:1508 20:1508 21:1508 22:1508 23:1509 24:1510 25:1510 26:1510 27:1510 28:1510 29:1510 30:1510 31:1510\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   token_is_max_context: 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_ids: 101 1999 3306 11327 2040 2001 1996 7804 1997 3500 3930 102 1037 3653 1011 3306 4761 1012 56 2566\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/30/2020 15:35:53 - INFO - src_nq.create_examples -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'mon', '##ism', 'pan', '##the', '##ism', 'or', '##th', '##op']\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'metis', 'mom', '##us', 'mor', '##pheus', 'nemesis', 'nike', 'pan']\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'places', '(', 'show', ')', 'el', '##eus', '##is', 'del']\n",
      "token list: ['[CLS]', 'in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth', '[SEP]', 'a', 'pre', '-', 'greek', 'origin', '.', '[unused55]', 'per']\n"
     ]
    }
   ],
   "source": [
    "examples = part\n",
    "is_training = True\n",
    "cached_path = \"\"\n",
    "unique_id = 1000000000\n",
    "\n",
    "features = []\n",
    "for (example_index, example) in enumerate(examples):\n",
    "    query_tokens = []\n",
    "    for token in example.question_tokens:\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        for sub_token in sub_tokens:\n",
    "            query_tokens.append(sub_token)\n",
    "\n",
    "    if len(query_tokens) > max_query_length:\n",
    "        query_tokens = query_tokens[-max_query_length:]\n",
    "\n",
    "    counts = {\"Table\": 0, \"Paragraph\": 0, \"List\": 0, \"Other\": 0}\n",
    "\n",
    "    sorted_la_candidates = sorted(enumerate(example.la_candidates), key=lambda x: x[1][\"start_token\"])\n",
    "\n",
    "    tok_is_sentence_end = []\n",
    "    tok_is_paragraph_end = []\n",
    "    tok_candidate_idx = []\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = [-1] * len(example.doc_tokens)\n",
    "    orig_to_tok_end_index = [-1] * len(example.doc_tokens)\n",
    "    all_doc_tokens = []\n",
    "\n",
    "    for candidate_idx, candidate in sorted_la_candidates:\n",
    "        if not candidate[\"top_level\"]:\n",
    "            continue\n",
    "        start_index = candidate[\"start_token\"]\n",
    "        end_index = candidate[\"end_token\"]\n",
    "\n",
    "        context_tokens = []\n",
    "        context_orig_idx = []\n",
    "\n",
    "        for tok_idx in range(start_index, end_index):\n",
    "            token_item = example.doc_tokens[tok_idx]\n",
    "            if not token_item[\"html_token\"]:\n",
    "                context_tokens.append(token_item[\"token\"].replace(\" \", \"\"))\n",
    "                if context_tokens[-1] == \"\":\n",
    "                    context_tokens = context_tokens[:-1]\n",
    "                    continue\n",
    "                context_orig_idx.append(tok_idx)\n",
    "\n",
    "        if len(context_tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        all_doc_tokens.append(\n",
    "            \"[unused{}]\".format(\n",
    "                get_candidate_type(example.doc_tokens[start_index][\"token\"], counts, max_position)))\n",
    "        tok_to_orig_index.append(-1)\n",
    "        tok_is_sentence_end.append(False)\n",
    "        tok_is_paragraph_end.append(False)\n",
    "        tok_candidate_idx.append(candidate_idx)\n",
    "\n",
    "        paragraph_len = 0\n",
    "        context = \" \".join(context_tokens)\n",
    "        context_sentences = nlp(context).sents\n",
    "        context_idx = 0\n",
    "        orig_to_tok_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "        for sentence in context_sentences:\n",
    "            sentence_len = 0\n",
    "            for token in sentence.string.strip().split():\n",
    "                if len(context_tokens[context_idx]) == 0:\n",
    "                    orig_to_tok_end_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "                    context_idx += 1\n",
    "                    orig_to_tok_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "                assert context_tokens[context_idx][:len(token)] == token\n",
    "                context_tokens[context_idx] = context_tokens[context_idx][len(token):]\n",
    "\n",
    "                sub_tokens = tokenizer.tokenize(token)\n",
    "\n",
    "                all_doc_tokens += sub_tokens\n",
    "                tok_to_orig_index += [context_orig_idx[context_idx]] * len(sub_tokens)\n",
    "                sentence_len += len(sub_tokens)\n",
    "                paragraph_len += len(sub_tokens)\n",
    "            if sentence_len > 0:\n",
    "                tok_is_sentence_end += [False] * (sentence_len - 1) + [True]\n",
    "\n",
    "        orig_to_tok_end_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "        assert context_idx + 1 == len(context_tokens)\n",
    "        assert context_tokens[context_idx] == \"\"\n",
    "\n",
    "        tok_is_paragraph_end += [False] * (paragraph_len - 1) + [True]\n",
    "        tok_candidate_idx += [candidate_idx] * paragraph_len\n",
    "\n",
    "    assert len(tok_is_sentence_end) == len(tok_is_paragraph_end)\n",
    "\n",
    "    la_tok_start_position = None\n",
    "    la_tok_end_position = None\n",
    "\n",
    "    sa_tok_start_position = None\n",
    "    sa_tok_end_position = None\n",
    "\n",
    "    example_answer_type = None\n",
    "\n",
    "    if is_training:\n",
    "        annotation = example.annotation[0]\n",
    "        if annotation[\"long_answer\"][\"start_token\"] != -1:\n",
    "            la_start_token = annotation[\"long_answer\"][\"start_token\"]\n",
    "            la_end_token = annotation[\"long_answer\"][\"end_token\"] - 1\n",
    "            while orig_to_tok_index[la_start_token] == -1:\n",
    "                la_start_token += 1\n",
    "            while orig_to_tok_end_index[la_end_token] == -1:\n",
    "                la_end_token -= 1\n",
    "\n",
    "            assert la_start_token <= la_end_token\n",
    "\n",
    "            la_tok_start_position = orig_to_tok_index[la_start_token]\n",
    "            la_tok_end_position = orig_to_tok_end_index[la_end_token] - 1\n",
    "\n",
    "            if annotation[\"yes_no_answer\"].lower() == \"none\":\n",
    "                example_answer_type = AnswerType.LONG\n",
    "            elif annotation[\"yes_no_answer\"].lower() == \"yes\":\n",
    "                example_answer_type = AnswerType.YES\n",
    "            elif annotation[\"yes_no_answer\"].lower() == \"no\":\n",
    "                example_answer_type = AnswerType.NO\n",
    "            else:\n",
    "                assert False\n",
    "\n",
    "            if \"short_answers\" in annotation and len(annotation[\"short_answers\"]) > 0:\n",
    "                sa_tok_start_position = len(all_doc_tokens) + 1\n",
    "                sa_tok_end_position = -1\n",
    "                example_answer_type = AnswerType.SHORT\n",
    "                for short_answer in annotation[\"short_answers\"]:\n",
    "                    sa_start_token = short_answer[\"start_token\"]\n",
    "                    sa_end_token = short_answer[\"end_token\"] - 1\n",
    "\n",
    "                    sa_tok_start_position = min(sa_tok_start_position, orig_to_tok_index[sa_start_token])\n",
    "                    sa_tok_end_position = max(sa_tok_end_position, orig_to_tok_end_index[sa_end_token] - 1)\n",
    "                    assert sa_tok_start_position >= 0 and sa_tok_end_position >= 0\n",
    "                assert la_tok_start_position <= sa_tok_start_position and la_tok_end_position >= sa_tok_end_position\n",
    "                la_tok_start_position = sa_tok_start_position\n",
    "                la_tok_end_position = sa_tok_end_position\n",
    "            else:\n",
    "                sa_tok_start_position = -1\n",
    "                sa_tok_end_position = -1\n",
    "        else:\n",
    "            example_answer_type = AnswerType.UNKNOWN\n",
    "            la_tok_start_position = -1\n",
    "            la_tok_end_position = -1\n",
    "            sa_tok_start_position = -1\n",
    "            sa_tok_end_position = -1\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    logger.info(\"ALL DOC_TOKENS %s\" %\" \".join([str(x) for x in all_doc_tokens][:50]))\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "        tokens = []\n",
    "        token_to_orig_map = {}\n",
    "        token_is_max_context = {}\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in query_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        is_sentence_end = [True] + [False] * (len(tokens) - 3) + [True] + [False]\n",
    "        is_paragraph_end = [True] + [False] * (len(tokens) - 3) + [True] + [False]\n",
    "        orig_tok_idx = [-1] * len(tokens)\n",
    "        candidate_idx = [-1] * len(tokens)\n",
    "\n",
    "        for i in range(doc_span.length):\n",
    "            split_token_index = doc_span.start + i\n",
    "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                    split_token_index)\n",
    "            token_is_max_context[len(tokens)] = is_max_context\n",
    "            tokens.append(all_doc_tokens[split_token_index])\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            is_sentence_end.append(tok_is_sentence_end[split_token_index])\n",
    "            is_paragraph_end.append(tok_is_paragraph_end[split_token_index])\n",
    "            orig_tok_idx.append(tok_to_orig_index[split_token_index])\n",
    "            candidate_idx.append(tok_candidate_idx[split_token_index])\n",
    "\n",
    "        is_sentence_end[-1] = False\n",
    "        is_paragraph_end[-1] = False\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        is_sentence_end.append(True)\n",
    "        is_paragraph_end.append(True)\n",
    "        orig_tok_idx.append(-1)\n",
    "        candidate_idx.append(-1)\n",
    "\n",
    "        assert len(orig_tok_idx) == len(tokens)\n",
    "        assert len(is_sentence_end) == len(tokens)\n",
    "        assert len(is_paragraph_end) == len(tokens)\n",
    "        assert len(candidate_idx) == len(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        la_start_position = None\n",
    "        la_end_position = None\n",
    "        sa_start_position = None\n",
    "        sa_end_position = None\n",
    "        answer_type = None\n",
    "        if is_training:\n",
    "            # For training, if our document chunk does not contain an annotation\n",
    "            # we throw it out, since there is nothing to predict.\n",
    "            doc_start = doc_span.start\n",
    "            doc_end = doc_span.start + doc_span.length - 1\n",
    "            out_of_span = False\n",
    "            if not (la_tok_start_position >= doc_start and\n",
    "                    la_tok_end_position <= doc_end):\n",
    "                out_of_span = True\n",
    "\n",
    "            if out_of_span:\n",
    "                if random.random() > include_unknowns:\n",
    "                    continue\n",
    "                answer_type = AnswerType.UNKNOWN\n",
    "                la_start_position = 0\n",
    "                la_end_position = 0\n",
    "                sa_start_position = 0\n",
    "                sa_end_position = 0\n",
    "            else:\n",
    "                answer_type = example_answer_type\n",
    "                doc_offset = len(query_tokens) + 2\n",
    "                la_start_position = la_tok_start_position - doc_start + doc_offset\n",
    "                la_end_position = la_tok_end_position - doc_start + doc_offset\n",
    "\n",
    "                if not (sa_tok_start_position >= doc_start and sa_tok_end_position <= doc_end):\n",
    "                    sa_start_position = -1\n",
    "                    sa_end_position = -1\n",
    "                else:\n",
    "                    sa_start_position = sa_tok_start_position - doc_start + doc_offset\n",
    "                    sa_end_position = sa_tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            assert la_start_position <= la_end_position\n",
    "            assert sa_start_position <= sa_end_position\n",
    "\n",
    "        if example_index >= 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (unique_id))\n",
    "            logger.info(\"example_index: %s\" % (example_index))\n",
    "            logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "            print(\"token list:\",tokens[:20])\n",
    "            # print(\"token list size\", len(tokens))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(tokens)[:20])\n",
    "            logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()][:20]))\n",
    "            logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "            ][:20]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids][:20]))\n",
    "            logger.info(\n",
    "                \"input_mask: %s\" % \" \".join([str(x) for x in input_mask][:20]))\n",
    "            logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids][:20]))\n",
    "            if is_training and answer_type != AnswerType.UNKNOWN:\n",
    "                answer_text = \" \".join(tokens[la_start_position:(la_end_position + 1)][:20])\n",
    "                logger.info(\"la_start_position: %d\" % (la_start_position))\n",
    "                logger.info(\"la_end_position: %d\" % (la_end_position))\n",
    "                logger.info(\n",
    "                    \"answer: %s\" % (answer_text))\n",
    "        doc_tree = get_doc_tree(is_sentence_end, is_paragraph_end, orig_tok_idx, candidate_idx)\n",
    "\n",
    "        graph, start_positions, end_positions = build_graph(doc_tree, len(is_sentence_end),\n",
    "                                                            la_start_position, la_end_position,\n",
    "                                                            sa_start_position, sa_end_position)\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=unique_id,\n",
    "                example_id=example.example_id,\n",
    "                doc_span_index=doc_span_index,\n",
    "                tokens=None,\n",
    "                token_to_orig_map=token_to_orig_map,\n",
    "                token_is_max_context=token_is_max_context,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                graph=graph,\n",
    "                answer_type=answer_type,\n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions))\n",
    "        unique_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(-1, [(-1, [(-1, 0)])]),\n",
       " (-1,\n",
       "  [(-1,\n",
       "    [(-1, 1),\n",
       "     (-1, 2),\n",
       "     (-1, 3),\n",
       "     (-1, 4),\n",
       "     (-1, 5),\n",
       "     (-1, 6),\n",
       "     (-1, 7),\n",
       "     (-1, 8),\n",
       "     (-1, 9),\n",
       "     (-1, 10)])]),\n",
       " (62,\n",
       "  [(-1,\n",
       "    [(-1, 11),\n",
       "     (1502, 12),\n",
       "     (1503, 13),\n",
       "     (1503, 14),\n",
       "     (1503, 15),\n",
       "     (1504, 16),\n",
       "     (1505, 17)])]),\n",
       " (63,\n",
       "  [(-1,\n",
       "    [(-1, 18),\n",
       "     (1508, 19),\n",
       "     (1508, 20),\n",
       "     (1508, 21),\n",
       "     (1508, 22),\n",
       "     (1509, 23),\n",
       "     (1510, 24),\n",
       "     (1510, 25),\n",
       "     (1510, 26),\n",
       "     (1510, 27),\n",
       "     (1510, 28),\n",
       "     (1510, 29),\n",
       "     (1510, 30),\n",
       "     (1510, 31),\n",
       "     (1510, 32),\n",
       "     (1510, 33),\n",
       "     (1511, 34),\n",
       "     (1512, 35),\n",
       "     (1513, 36),\n",
       "     (1514, 37),\n",
       "     (1515, 38),\n",
       "     (1516, 39),\n",
       "     (1516, 40),\n",
       "     (1517, 41),\n",
       "     (1518, 42),\n",
       "     (1518, 43),\n",
       "     (1518, 44),\n",
       "     (1519, 45),\n",
       "     (1520, 46),\n",
       "     (1521, 47),\n",
       "     (1521, 48),\n",
       "     (1522, 49)]),\n",
       "   (-1,\n",
       "    [(1523, 50),\n",
       "     (1524, 51),\n",
       "     (1525, 52),\n",
       "     (1526, 53),\n",
       "     (1527, 54),\n",
       "     (1528, 55),\n",
       "     (1529, 56),\n",
       "     (1530, 57),\n",
       "     (1531, 58),\n",
       "     (1532, 59),\n",
       "     (1533, 60),\n",
       "     (1534, 61),\n",
       "     (1534, 62),\n",
       "     (1535, 63),\n",
       "     (1535, 64),\n",
       "     (1536, 65),\n",
       "     (1537, 66),\n",
       "     (1537, 67),\n",
       "     (1538, 68),\n",
       "     (1539, 69),\n",
       "     (1540, 70),\n",
       "     (1541, 71),\n",
       "     (1542, 72),\n",
       "     (1542, 73),\n",
       "     (1543, 74),\n",
       "     (1543, 75),\n",
       "     (1544, 76),\n",
       "     (1545, 77),\n",
       "     (1545, 78),\n",
       "     (1546, 79),\n",
       "     (1547, 80),\n",
       "     (1548, 81),\n",
       "     (1548, 82),\n",
       "     (1549, 83),\n",
       "     (1549, 84),\n",
       "     (1550, 85),\n",
       "     (1551, 86),\n",
       "     (1552, 87),\n",
       "     (1552, 88),\n",
       "     (1553, 89),\n",
       "     (1554, 90),\n",
       "     (1555, 91),\n",
       "     (1556, 92),\n",
       "     (1557, 93),\n",
       "     (1558, 94),\n",
       "     (1559, 95),\n",
       "     (1560, 96),\n",
       "     (1561, 97),\n",
       "     (1562, 98),\n",
       "     (1563, 99),\n",
       "     (1564, 100),\n",
       "     (1565, 101),\n",
       "     (1566, 102),\n",
       "     (1567, 103),\n",
       "     (1568, 104),\n",
       "     (1568, 105),\n",
       "     (1569, 106),\n",
       "     (1570, 107),\n",
       "     (1571, 108),\n",
       "     (1572, 109),\n",
       "     (1572, 110),\n",
       "     (1573, 111),\n",
       "     (1574, 112),\n",
       "     (1575, 113),\n",
       "     (1576, 114),\n",
       "     (1577, 115),\n",
       "     (1578, 116),\n",
       "     (1578, 117),\n",
       "     (1578, 118),\n",
       "     (1579, 119),\n",
       "     (1580, 120),\n",
       "     (1580, 121),\n",
       "     (1581, 122),\n",
       "     (1582, 123),\n",
       "     (1583, 124),\n",
       "     (1583, 125),\n",
       "     (1584, 126)])]),\n",
       " (64,\n",
       "  [(-1,\n",
       "    [(-1, 127),\n",
       "     (1587, 128),\n",
       "     (1588, 129),\n",
       "     (1589, 130),\n",
       "     (1590, 131),\n",
       "     (1591, 132),\n",
       "     (1592, 133),\n",
       "     (1592, 134),\n",
       "     (1592, 135),\n",
       "     (1592, 136),\n",
       "     (1592, 137),\n",
       "     (1592, 138),\n",
       "     (1593, 139),\n",
       "     (1593, 140),\n",
       "     (1593, 141),\n",
       "     (1593, 142),\n",
       "     (1593, 143),\n",
       "     (1594, 144),\n",
       "     (1595, 145),\n",
       "     (1595, 146),\n",
       "     (1595, 147),\n",
       "     (1596, 148),\n",
       "     (1596, 149),\n",
       "     (1596, 150),\n",
       "     (1597, 151),\n",
       "     (1598, 152),\n",
       "     (1598, 153),\n",
       "     (1599, 154),\n",
       "     (1600, 155),\n",
       "     (1601, 156),\n",
       "     (1602, 157),\n",
       "     (1603, 158),\n",
       "     (1604, 159),\n",
       "     (1605, 160),\n",
       "     (1606, 161),\n",
       "     (1606, 162),\n",
       "     (1607, 163)])]),\n",
       " (65,\n",
       "  [(-1,\n",
       "    [(-1, 164),\n",
       "     (1654, 165),\n",
       "     (1655, 166),\n",
       "     (1656, 167),\n",
       "     (1657, 168),\n",
       "     (1658, 169),\n",
       "     (1659, 170),\n",
       "     (1660, 171),\n",
       "     (1661, 172),\n",
       "     (1662, 173),\n",
       "     (1662, 174),\n",
       "     (1662, 175),\n",
       "     (1663, 176),\n",
       "     (1664, 177),\n",
       "     (1665, 178),\n",
       "     (1666, 179),\n",
       "     (1667, 180),\n",
       "     (1668, 181),\n",
       "     (1668, 182),\n",
       "     (1668, 183),\n",
       "     (1669, 184),\n",
       "     (1670, 185),\n",
       "     (1671, 186),\n",
       "     (1672, 187),\n",
       "     (1673, 188),\n",
       "     (1673, 189),\n",
       "     (1674, 190),\n",
       "     (1675, 191),\n",
       "     (1675, 192),\n",
       "     (1675, 193),\n",
       "     (1676, 194),\n",
       "     (1677, 195),\n",
       "     (1677, 196),\n",
       "     (1677, 197),\n",
       "     (1677, 198),\n",
       "     (1677, 199),\n",
       "     (1677, 200),\n",
       "     (1677, 201),\n",
       "     (1677, 202),\n",
       "     (1677, 203),\n",
       "     (1677, 204),\n",
       "     (1678, 205),\n",
       "     (1679, 206)]),\n",
       "   (-1,\n",
       "    [(1680, 207),\n",
       "     (1681, 208),\n",
       "     (1682, 209),\n",
       "     (1683, 210),\n",
       "     (1684, 211),\n",
       "     (1685, 212),\n",
       "     (1686, 213),\n",
       "     (1687, 214),\n",
       "     (1688, 215),\n",
       "     (1688, 216),\n",
       "     (1688, 217),\n",
       "     (1689, 218),\n",
       "     (1690, 219),\n",
       "     (1691, 220),\n",
       "     (1692, 221),\n",
       "     (1693, 222),\n",
       "     (1694, 223),\n",
       "     (1695, 224),\n",
       "     (1696, 225),\n",
       "     (1697, 226),\n",
       "     (1698, 227),\n",
       "     (1698, 228),\n",
       "     (1698, 229),\n",
       "     (1699, 230),\n",
       "     (1700, 231),\n",
       "     (1700, 232),\n",
       "     (1701, 233),\n",
       "     (1702, 234),\n",
       "     (1703, 235),\n",
       "     (1704, 236),\n",
       "     (1704, 237),\n",
       "     (1705, 238),\n",
       "     (1706, 239),\n",
       "     (1707, 240),\n",
       "     (1708, 241),\n",
       "     (1709, 242),\n",
       "     (1710, 243),\n",
       "     (1710, 244),\n",
       "     (1711, 245),\n",
       "     (1712, 246),\n",
       "     (1713, 247),\n",
       "     (1714, 248),\n",
       "     (1715, 249)])]),\n",
       " (66,\n",
       "  [(-1,\n",
       "    [(-1, 250),\n",
       "     (1718, 251),\n",
       "     (1719, 252),\n",
       "     (1719, 253),\n",
       "     (1719, 254),\n",
       "     (1720, 255),\n",
       "     (1721, 256),\n",
       "     (1722, 257),\n",
       "     (1723, 258),\n",
       "     (1724, 259),\n",
       "     (1724, 260),\n",
       "     (1724, 261),\n",
       "     (1725, 262),\n",
       "     (1726, 263),\n",
       "     (1727, 264),\n",
       "     (1728, 265),\n",
       "     (1729, 266),\n",
       "     (1730, 267),\n",
       "     (1731, 268),\n",
       "     (1732, 269),\n",
       "     (1733, 270),\n",
       "     (1734, 271),\n",
       "     (1735, 272),\n",
       "     (1736, 273),\n",
       "     (1736, 274),\n",
       "     (1737, 275)]),\n",
       "   (-1,\n",
       "    [(1738, 276),\n",
       "     (1739, 277),\n",
       "     (1740, 278),\n",
       "     (1740, 279),\n",
       "     (1741, 280),\n",
       "     (1742, 281),\n",
       "     (1742, 282),\n",
       "     (1743, 283),\n",
       "     (1744, 284),\n",
       "     (1745, 285),\n",
       "     (1745, 286),\n",
       "     (1745, 287),\n",
       "     (1746, 288),\n",
       "     (1747, 289),\n",
       "     (1748, 290),\n",
       "     (1749, 291),\n",
       "     (1750, 292),\n",
       "     (1751, 293),\n",
       "     (1752, 294),\n",
       "     (1753, 295),\n",
       "     (1754, 296),\n",
       "     (1755, 297),\n",
       "     (1756, 298),\n",
       "     (1757, 299),\n",
       "     (1758, 300),\n",
       "     (1759, 301),\n",
       "     (1760, 302),\n",
       "     (1761, 303),\n",
       "     (1762, 304),\n",
       "     (1763, 305),\n",
       "     (1764, 306),\n",
       "     (1764, 307),\n",
       "     (1764, 308),\n",
       "     (1765, 309),\n",
       "     (1766, 310),\n",
       "     (1767, 311),\n",
       "     (1768, 312),\n",
       "     (1768, 313),\n",
       "     (1768, 314),\n",
       "     (1769, 315),\n",
       "     (1770, 316),\n",
       "     (1771, 317),\n",
       "     (1771, 318),\n",
       "     (1772, 319),\n",
       "     (1773, 320),\n",
       "     (1774, 321),\n",
       "     (1775, 322),\n",
       "     (1776, 323),\n",
       "     (1777, 324),\n",
       "     (1778, 325),\n",
       "     (1778, 326),\n",
       "     (1779, 327),\n",
       "     (1780, 328),\n",
       "     (1781, 329),\n",
       "     (1782, 330)])]),\n",
       " (67,\n",
       "  [(-1,\n",
       "    [(-1, 331),\n",
       "     (1791, 332),\n",
       "     (1792, 333),\n",
       "     (1793, 334),\n",
       "     (1794, 335),\n",
       "     (1795, 336),\n",
       "     (1796, 337),\n",
       "     (1797, 338),\n",
       "     (1798, 339),\n",
       "     (1798, 340),\n",
       "     (1798, 341),\n",
       "     (1798, 342),\n",
       "     (1799, 343),\n",
       "     (1800, 344),\n",
       "     (1800, 345),\n",
       "     (1801, 346),\n",
       "     (1802, 347),\n",
       "     (1802, 348),\n",
       "     (1803, 349),\n",
       "     (1804, 350),\n",
       "     (1805, 351),\n",
       "     (1806, 352),\n",
       "     (1807, 353),\n",
       "     (1808, 354),\n",
       "     (1809, 355),\n",
       "     (1810, 356),\n",
       "     (1811, 357),\n",
       "     (1812, 358),\n",
       "     (1813, 359),\n",
       "     (1814, 360),\n",
       "     (1815, 361),\n",
       "     (1816, 362),\n",
       "     (1817, 363),\n",
       "     (1818, 364),\n",
       "     (1819, 365),\n",
       "     (1819, 366),\n",
       "     (1820, 367),\n",
       "     (1821, 368),\n",
       "     (1822, 369),\n",
       "     (1823, 370),\n",
       "     (1824, 371),\n",
       "     (1825, 372),\n",
       "     (1825, 373),\n",
       "     (1825, 374),\n",
       "     (1826, 375)]),\n",
       "   (-1,\n",
       "    [(1827, 376),\n",
       "     (1827, 377),\n",
       "     (1828, 378),\n",
       "     (1829, 379),\n",
       "     (1830, 380),\n",
       "     (1831, 381),\n",
       "     (1831, 382),\n",
       "     (1832, 383),\n",
       "     (1833, 384),\n",
       "     (1834, 385),\n",
       "     (1835, 386)]),\n",
       "   (-1,\n",
       "    [(1836, 387),\n",
       "     (1836, 388),\n",
       "     (1836, 389),\n",
       "     (1836, 390),\n",
       "     (1837, 391),\n",
       "     (1837, 392),\n",
       "     (1838, 393),\n",
       "     (1839, 394),\n",
       "     (1840, 395),\n",
       "     (1841, 396),\n",
       "     (1842, 397),\n",
       "     (1843, 398)]),\n",
       "   (-1,\n",
       "    [(1844, 399),\n",
       "     (1845, 400),\n",
       "     (1845, 401),\n",
       "     (1846, 402),\n",
       "     (1847, 403),\n",
       "     (1847, 404),\n",
       "     (1848, 405),\n",
       "     (1849, 406),\n",
       "     (1850, 407),\n",
       "     (1851, 408),\n",
       "     (1852, 409),\n",
       "     (1853, 410),\n",
       "     (1853, 411)])]),\n",
       " (68,\n",
       "  [(-1,\n",
       "    [(-1, 412),\n",
       "     (1856, 413),\n",
       "     (1857, 414),\n",
       "     (1858, 415),\n",
       "     (1859, 416),\n",
       "     (1860, 417),\n",
       "     (1861, 418),\n",
       "     (1861, 419),\n",
       "     (1861, 420),\n",
       "     (1861, 421),\n",
       "     (1862, 422),\n",
       "     (1863, 423),\n",
       "     (1864, 424),\n",
       "     (1865, 425),\n",
       "     (1866, 426),\n",
       "     (1867, 427),\n",
       "     (1868, 428),\n",
       "     (1869, 429),\n",
       "     (1870, 430),\n",
       "     (1870, 431),\n",
       "     (1870, 432),\n",
       "     (1871, 433),\n",
       "     (1872, 434),\n",
       "     (1873, 435),\n",
       "     (1874, 436),\n",
       "     (1875, 437),\n",
       "     (1875, 438),\n",
       "     (1876, 439),\n",
       "     (1876, 440),\n",
       "     (1877, 441),\n",
       "     (1878, 442),\n",
       "     (1879, 443),\n",
       "     (1879, 444),\n",
       "     (1879, 445),\n",
       "     (1879, 446),\n",
       "     (1880, 447),\n",
       "     (1881, 448),\n",
       "     (1882, 449),\n",
       "     (1882, 450),\n",
       "     (1883, 451),\n",
       "     (1884, 452),\n",
       "     (1885, 453),\n",
       "     (1886, 454),\n",
       "     (1887, 455),\n",
       "     (1888, 456),\n",
       "     (1889, 457),\n",
       "     (1890, 458),\n",
       "     (1891, 459),\n",
       "     (1892, 460),\n",
       "     (1893, 461),\n",
       "     (1894, 462),\n",
       "     (1895, 463),\n",
       "     (1896, 464),\n",
       "     (1897, 465),\n",
       "     (1898, 466),\n",
       "     (1899, 467),\n",
       "     (1900, 468),\n",
       "     (1901, 469),\n",
       "     (1902, 470),\n",
       "     (1903, 471),\n",
       "     (1904, 472),\n",
       "     (1905, 473),\n",
       "     (1905, 474),\n",
       "     (1905, 475),\n",
       "     (1905, 476),\n",
       "     (1905, 477),\n",
       "     (1906, 478),\n",
       "     (1907, 479),\n",
       "     (1908, 480),\n",
       "     (1909, 481),\n",
       "     (1909, 482),\n",
       "     (1910, 483),\n",
       "     (1911, 484),\n",
       "     (1911, 485),\n",
       "     (1912, 486),\n",
       "     (1913, 487),\n",
       "     (1914, 488),\n",
       "     (1914, 489),\n",
       "     (1915, 490),\n",
       "     (1916, 491),\n",
       "     (1917, 492),\n",
       "     (1917, 493),\n",
       "     (1918, 494),\n",
       "     (1919, 495),\n",
       "     (1920, 496),\n",
       "     (1921, 497),\n",
       "     (1922, 498),\n",
       "     (1923, 499),\n",
       "     (1924, 500),\n",
       "     (1925, 501),\n",
       "     (1926, 502),\n",
       "     (1927, 503),\n",
       "     (1928, 504)])]),\n",
       " (69,\n",
       "  [(-1,\n",
       "    [(-1, 505),\n",
       "     (1939, 506),\n",
       "     (1940, 507),\n",
       "     (1940, 508),\n",
       "     (1941, 509),\n",
       "     (1942, 510),\n",
       "     (-1, 511)])])]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "segment_ids\n",
    "tokens\n",
    "input_ids\n",
    "doc_tree\n",
    "is_sentence_end\n",
    "is_paragraph_end\n",
    "example.annotation\n",
    "candidate_idx\n",
    "orig_tok_idx\n",
    "doc_tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "source": [
    "doc_spans\n",
    "example = part[0]\n",
    "example.doc_tokens[0:20]\n",
    "# [x for x in example.doc_tokens if x['token']=='episode']\n",
    "[x for x in example.doc_tokens if x['start_byte']==3113]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n['in', 'greek', 'mythology', 'who', 'was', 'the', 'goddess', 'of', 'spring', 'growth']\n['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n['benefits', 'of', 'colonial', 'life', 'for', 'single', 'celled', 'organisms']\n['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n['how', 'many', 'season', 'of', 'the', 'man', 'in', 'the', 'high', 'castle']\n['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n['who', 'was', 'the', 'first', 'ministry', 'head', 'of', 'state', 'in', 'nigeria']\n['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(part[i].question_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000001"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "features[1].unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4549465242785278785"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "features[1].example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "features[1].doc_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[1].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[397, 13, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "# features[1].token_to_orig_map\n",
    "# features[1].token_is_max_context\n",
    "# features[1].segment_ids\n",
    "# features[1].answer_type\n",
    "features[1].start_positions\n",
    "features[1].end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['hello', 'yo', '##ho', 'sb']"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hello yoho sb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0,\n",
       " {'end_byte': 57620,\n",
       "  'end_token': 216,\n",
       "  'start_byte': 53609,\n",
       "  'start_token': 24,\n",
       "  'top_level': True})"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "example = examples[0]\n",
    "sorted_la_candidates = sorted(enumerate(example.la_candidates), key=lambda x: x[1][\"start_token\"])\n",
    "sorted_la_candidates[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = sorted_la_candidates[0][1]\n",
    "start_index = candidate[\"start_token\"]\n",
    "end_index = candidate[\"end_token\"]\n",
    "\n",
    "context_tokens = []\n",
    "context_orig_idx = []\n",
    "\n",
    "for tok_idx in range(start_index, end_index):\n",
    "    token_item = example.doc_tokens[tok_idx]\n",
    "    if not token_item[\"html_token\"]:\n",
    "        context_tokens.append(token_item[\"token\"].replace(\" \", \"\"))\n",
    "        if context_tokens[-1] == \"\":\n",
    "            context_tokens = context_tokens[:-1]\n",
    "            continue\n",
    "        context_orig_idx.append(tok_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'end_byte': 53868,\n",
       " 'html_token': False,\n",
       " 'start_byte': 53862,\n",
       " 'token': 'season'}"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "example.doc_tokens[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Walking',\n",
       " 'Dead',\n",
       " '(',\n",
       " 'season',\n",
       " '8',\n",
       " ')',\n",
       " 'Promotional',\n",
       " 'poster',\n",
       " 'Starring',\n",
       " 'Andrew',\n",
       " 'Lincoln',\n",
       " 'Norman',\n",
       " 'Reedus',\n",
       " 'Lauren',\n",
       " 'Cohan',\n",
       " 'Chandler',\n",
       " 'Riggs',\n",
       " 'Danai',\n",
       " 'Gurira']"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "context_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 38,\n",
       " 39,\n",
       " 44,\n",
       " 49,\n",
       " 50,\n",
       " 53,\n",
       " 54,\n",
       " 57,\n",
       " 58,\n",
       " 61,\n",
       " 62,\n",
       " 65,\n",
       " 66]"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "context_orig_idx[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\"Table\": 0, \"Paragraph\": 0, \"List\": 0, \"Other\": 0}\n",
    "\n",
    "sorted_la_candidates = sorted(enumerate(example.la_candidates), key=lambda x: x[1][\"start_token\"])\n",
    "\n",
    "tok_is_sentence_end = []\n",
    "tok_is_paragraph_end = []\n",
    "tok_candidate_idx = []\n",
    "tok_to_orig_index = []\n",
    "orig_to_tok_index = [-1] * len(example.doc_tokens)\n",
    "orig_to_tok_end_index = [-1] * len(example.doc_tokens)\n",
    "all_doc_tokens = []\n",
    "\n",
    "for candidate_idx, candidate in sorted_la_candidates:\n",
    "    if not candidate[\"top_level\"]:\n",
    "        continue\n",
    "    start_index = candidate[\"start_token\"]\n",
    "    end_index = candidate[\"end_token\"]\n",
    "\n",
    "    context_tokens = []\n",
    "    context_orig_idx = []\n",
    "\n",
    "    for tok_idx in range(start_index, end_index):\n",
    "        token_item = example.doc_tokens[tok_idx]\n",
    "        if not token_item[\"html_token\"]:\n",
    "            context_tokens.append(token_item[\"token\"].replace(\" \", \"\"))\n",
    "            if context_tokens[-1] == \"\":\n",
    "                context_tokens = context_tokens[:-1]\n",
    "                continue\n",
    "            context_orig_idx.append(tok_idx)\n",
    "\n",
    "    if len(context_tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    all_doc_tokens.append(\n",
    "        \"[unused{}]\".format(\n",
    "            get_candidate_type(example.doc_tokens[start_index][\"token\"], counts, max_position)))\n",
    "    tok_to_orig_index.append(-1)\n",
    "    tok_is_sentence_end.append(False)\n",
    "    tok_is_paragraph_end.append(False)\n",
    "    tok_candidate_idx.append(candidate_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['^',\n",
       " '1',\n",
       " 'Live',\n",
       " '+',\n",
       " '7',\n",
       " 'ratings',\n",
       " 'were',\n",
       " 'not',\n",
       " 'available',\n",
       " ',',\n",
       " 'so',\n",
       " 'Live',\n",
       " '+',\n",
       " '3',\n",
       " 'ratings',\n",
       " 'have',\n",
       " 'been',\n",
       " 'used',\n",
       " 'instead',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "xs = [ca for ca in sorted_la_candidates if ca[1][\"top_level\"]]\n",
    "len(xs)\n",
    "all_doc_tokens\n",
    "tok_is_sentence_end\n",
    "tok_is_paragraph_end\n",
    "tok_candidate_idx\n",
    "context_tokens\n",
    "context = \" \".join(context_tokens)\n",
    "context\n",
    "context_sentences = nlp(context).sents\n",
    "context_idx = 0\n",
    "orig_to_tok_index[context_orig_idx[context_idx]] = len(all_doc_tokens)\n",
    "for sentence in context_sentences:\n",
    "    sentence_len = 0\n",
    "    for token in sentence.string.strip().split():\n",
    "        # print(token)\n",
    "        pass\n",
    "\n",
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(4549465242785278785, 21),\n",
       " (4549465242785278785, 22),\n",
       " (4549465242785278785, 23),\n",
       " (4549465242785278785, 24),\n",
       " (4549465242785278785, 22),\n",
       " (4549465242785278785, 23),\n",
       " (4549465242785278785, 24),\n",
       " (-2543388002166163252, 2),\n",
       " (-2543388002166163252, 3),\n",
       " (-2543388002166163252, 4),\n",
       " (-2543388002166163252, 8),\n",
       " (-2543388002166163252, 35),\n",
       " (-2543388002166163252, 63),\n",
       " (4549465242785278785, 22),\n",
       " (4549465242785278785, 23),\n",
       " (4549465242785278785, 24),\n",
       " (4549465242785278785, 5),\n",
       " (4549465242785278785, 22),\n",
       " (4549465242785278785, 23),\n",
       " (4549465242785278785, 24)]"
      ]
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "source": [
    "[(features[i].example_id, features[i].doc_span_index) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[DocSpan(start=0, length=497),\n",
       " DocSpan(start=128, length=497),\n",
       " DocSpan(start=256, length=497),\n",
       " DocSpan(start=384, length=497),\n",
       " DocSpan(start=512, length=497),\n",
       " DocSpan(start=640, length=497),\n",
       " DocSpan(start=768, length=497),\n",
       " DocSpan(start=896, length=497),\n",
       " DocSpan(start=1024, length=497),\n",
       " DocSpan(start=1152, length=497),\n",
       " DocSpan(start=1280, length=497),\n",
       " DocSpan(start=1408, length=497),\n",
       " DocSpan(start=1536, length=497),\n",
       " DocSpan(start=1664, length=497),\n",
       " DocSpan(start=1792, length=497),\n",
       " DocSpan(start=1920, length=497),\n",
       " DocSpan(start=2048, length=497),\n",
       " DocSpan(start=2176, length=497),\n",
       " DocSpan(start=2304, length=497),\n",
       " DocSpan(start=2432, length=497),\n",
       " DocSpan(start=2560, length=497),\n",
       " DocSpan(start=2688, length=497),\n",
       " DocSpan(start=2816, length=497),\n",
       " DocSpan(start=2944, length=497),\n",
       " DocSpan(start=3072, length=497),\n",
       " DocSpan(start=3200, length=497),\n",
       " DocSpan(start=3328, length=461)]"
      ]
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "source": [
    "doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}